{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4eb5f2-eb6f-407f-a95a-fd9f47ca6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7953ca-023b-4a97-b202-fa522c9a4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SINGLE VARIABLE LINEAR REGRESSION\n",
    "\n",
    "def pick_weights_and_biases():\n",
    "    w = np.random.rand()\n",
    "    b = np.random.rand()\n",
    "    return w, b\n",
    "\n",
    "def calculate_MSE(X, Y, w, b):\n",
    "    cost = 0\n",
    "    m = len(X) #No. of training example\n",
    "    for i in range(m):\n",
    "        f_wb = (w * X[i]) + b\n",
    "        cost += (f_wb - Y[i]) ** 2\n",
    "    cost = cost / m\n",
    "    \n",
    "    # Using Numpy to remove the use of the redundant loop\n",
    "    # m = len(X)\n",
    "    # predictions = w * X + b\n",
    "    # cost = np.mean((predictions - Y) ** 2)\n",
    "    return cost\n",
    "\n",
    "def calculate_gradient(X, Y, w, b):\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    m = len(X)\n",
    "    for i in range(m):\n",
    "        f_wb = (w * X[i]) + b\n",
    "        dj_dw_i = (f_wb - Y[i]) * X[i]\n",
    "        dj_db_i = (f_wb - Y[i])\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    \n",
    "    # Using Numpy to remove redundant loops\n",
    "    # m = len(X)\n",
    "    # predictions = w * X + b\n",
    "    # errors = predictions - Y\n",
    "    # dj_dw = (1/m) * np.dot(errors, X)  # Gradient with respect to w\n",
    "    # dj_db = (1/m) * np.sum(errors)      # Gradient with respect to b\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent(X, Y, w, b, alpha, iterations):\n",
    "    for i in range(iterations):\n",
    "        dj_dw, dj_db = calculate_gradient(X, Y, w, b)\n",
    "        w = w - (alpha * dj_dw)\n",
    "        b = b - (alpha * dj_db)\n",
    "        if i%100 == 0:\n",
    "            print(f\" i = {i} ---> cost = {calculate_MSE(X, Y, w, b)}\")\n",
    "    return w, b\n",
    "\n",
    "def predict(X, w, b):\n",
    "    f_wb = w * X + b\n",
    "    return f_wb # Return the predicted value (a continuous value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af35817-1890-45b4-b48c-98e96b561708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SINGLE VARIABLE LOGISTIC REGRESSION\n",
    "\n",
    "def pick_weights_and_biases():\n",
    "    w = np.random.rand()\n",
    "    b = np.random.rand()\n",
    "    return w, b\n",
    "\n",
    "def sigmoid(z):\n",
    "    return (1/(1 + np.exp(-z)))\n",
    "\n",
    "def calculate_cost(X, Y, w, b):\n",
    "    cost = 0\n",
    "    m = len(X)\n",
    "    for i in range(m):\n",
    "        z = (w * X[i]) + b\n",
    "        f_wb = sigmoid(z)\n",
    "        cost += (-Y[i] * np.log(f_wb)) - ((1 - Y[i]) * np.log(1 - f_wb))\n",
    "    cost = cost / m\n",
    "\n",
    "    # Using Numpy\n",
    "    # m = len(X)\n",
    "    # z = w * X + b\n",
    "    # f_wb = sigmoid(z)\n",
    "    # cost = -np.mean(Y * np.log(f_wb) + (1 - Y) * np.log(1 - f_wb))\n",
    "    return cost\n",
    "\n",
    "def calculate_gradient(X, Y, w, b):\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    m = len(X)\n",
    "    for i in range(m):\n",
    "        z = (w * X[i]) + b\n",
    "        f_wb = sigmoid(z)\n",
    "        dj_dw_i = (f_wb - Y[i]) * X[i]\n",
    "        dj_db_i = (f_wb - Y[i])\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    \n",
    "    # Using Numpy to remove redundant loops\n",
    "    # m = len(X)\n",
    "    # z = w * X + b\n",
    "    # f_wb = sigmoid(z)\n",
    "    # errors = f_wb - Y\n",
    "    # dj_dw = (1/m) * np.dot(errors, X)  # Gradient with respect to w\n",
    "    # dj_db = (1/m) * np.sum(errors)      # Gradient with respect to b\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent(X, Y, w, b, alpha, iterations):\n",
    "    for i in range(iterations):\n",
    "        dj_dw, dj_db = calculate_gradient(X, Y, w, b)\n",
    "        w = w - (alpha * dj_dw)\n",
    "        b = b - (alpha * dj_db)\n",
    "        if i%100 == 0:\n",
    "            print(f\" i = {i} ---> cost = {calculate_cost(X, Y, w, b)}\")\n",
    "    return w, b\n",
    "\n",
    "def predict_single_variable_logistic(X, w, b):\n",
    "    f_wb = sigmoid(w * X + b)\n",
    "    return f_wb >= 0.5  # Return True (1) if probability >= 0.5, else False (0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50eca8af-a6df-4d32-b12a-a6343b24a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULTI VARIABLE LINEAR REGRESSION\n",
    "\n",
    "def pick_weights_and_biases(features):\n",
    "    n = features.shape[1]  # Get the number of features (columns)\n",
    "    w = np.random.randn(n)\n",
    "    b = np.random.randn()\n",
    "    return w, b\n",
    "\n",
    "def calculate_MSE(X, Y, w, b):\n",
    "    m = len(X)\n",
    "    f_wb = np.dot(X, w) + b\n",
    "    cost = np.sum((f_wb - Y) ** 2) / (2 * m)  # Mean Squared Error\n",
    "    return cost\n",
    "\n",
    "def calculate_MAE(X, Y, w, b):\n",
    "    m = len(X)\n",
    "    f_wb = np.dot(X, w) + b\n",
    "    cost = np.sum(np.abs(f_wb - Y)) / m  # Mean Absolute Error\n",
    "    return cost\n",
    "\n",
    "def calculate_gradient(X, Y, w, b):\n",
    "    m = len(X)\n",
    "    f_wb = np.dot(X, w) + b\n",
    "    dj_dw = np.dot(X.T, (f_wb - Y)) / m  # Gradient with respect to w\n",
    "    dj_db = np.sum(f_wb - Y) / m        # Gradient with respect to b\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent(X, Y, w, b, alpha, iterations):\n",
    "    for i in range(iterations):\n",
    "        dj_dw, dj_db = calculate_gradient(X, Y, w, b)\n",
    "        w -= alpha * dj_dw\n",
    "        b -= alpha * dj_db\n",
    "        if i % (iterations // 10) == 0:  # Print every 10% of iterations\n",
    "            print(f\"Iteration {i} ---> Absolute cost = {calculate_MAE(X, Y, w, b)}\")\n",
    "            print(f\"Iteration {i} ---> Squared cost = {calculate_MSE(X, Y, w, b)}\")\n",
    "    return w, b\n",
    "\n",
    "def predict_multivariable_linear(X, w, b):\n",
    "    f_wb = np.dot(X, w) + b\n",
    "    return f_wb  # Return the predicted value (a continuous value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1872a7d0-df0f-4394-9871-4cae3d71e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULTI VARIABLE LOGISTIC REGRESSION\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def pick_weights_and_biases(features):\n",
    "    n = features.shape[1]  # Get the number of features (columns)\n",
    "    w = np.random.randn(n)\n",
    "    b = np.random.randn()\n",
    "    return w, b\n",
    "\n",
    "def calculate_MSE(X, Y, w, b):\n",
    "    m = len(X)\n",
    "    f_wb = sigmoid(np.dot(X, w) + b)  # Logistic prediction\n",
    "    cost = -(1/m) * np.sum(Y * np.log(f_wb) + (1 - Y) * np.log(1 - f_wb))\n",
    "    return cost\n",
    "\n",
    "def calculate_gradient(X, Y, w, b):\n",
    "    m = len(X)\n",
    "    f_wb = sigmoid(np.dot(X, w) + b)  # Logistic prediction\n",
    "    dj_dw = np.dot(X.T, (f_wb - Y)) / m  # Gradient with respect to w\n",
    "    dj_db = np.sum(f_wb - Y) / m       # Gradient with respect to b\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent(X, Y, w, b, alpha, iterations):\n",
    "    for i in range(iterations):\n",
    "        dj_dw, dj_db = calculate_gradient(X, Y, w, b)\n",
    "        w -= alpha * dj_dw  # Update weights\n",
    "        b -= alpha * dj_db  # Update bias\n",
    "        if i % (iterations // 10) == 0:\n",
    "            print(f\"Iteration {i} ---> Cost = {calculate_cost(X, Y, w, b)}\")\n",
    "    return w, b\n",
    "\n",
    "def predict(X, w, b):\n",
    "    f_wb = sigmoid(np.dot(X, w) + b)\n",
    "    return f_wb >= 0.5 # Return True (1) if probability >= 0.5, else False (0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
